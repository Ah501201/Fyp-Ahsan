{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23839 images belonging to 6 classes.\n",
      "Found 5850 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'D:/images/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'D:/images/validation',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 21, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 32,918\n",
      "Trainable params: 32,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Fused conv implementation does not support grouped convolutions for now.\n\t [[node sequential_6/conv2d_15/Relu (defined at <ipython-input-41-4b9cf3b8c9b2>:30) ]] [Op:__inference_train_function_7803]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4b9cf3b8c9b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         validation_steps=400,shuffle=True)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#-------------------Save the model on device------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Fused conv implementation does not support grouped convolutions for now.\n\t [[node sequential_6/conv2d_15/Relu (defined at <ipython-input-41-4b9cf3b8c9b2>:30) ]] [Op:__inference_train_function_7803]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        #Dropout(0.5)\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "Dropout(0.2)\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "        #-----------set model compilation attributes------------------------\n",
    "        \n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        #--------to just see  the Model detials-----------------------------\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    steps_per_epoch=len(X_train) / batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "history=model.fit(\n",
    "        train_generator.flow(X_train, y_train, batch_size=batch_size),\n",
    "        steps_per_epoch=1000,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=400,shuffle=True)\n",
    "\n",
    "    #-------------------Save the model on device------------------------\n",
    "model.save(\"CNN_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3993/3993 [00:02<00:00, 1799.91it/s]\n",
      "100%|██████████| 436/436 [00:00<00:00, 2086.76it/s]\n",
      "100%|██████████| 4103/4103 [00:02<00:00, 1733.69it/s]\n",
      "100%|██████████| 7164/7164 [00:03<00:00, 1945.91it/s]\n",
      "100%|██████████| 4938/4938 [00:02<00:00, 1720.38it/s]\n",
      "100%|██████████| 3205/3205 [00:01<00:00, 1697.61it/s]\n",
      " 17%|█▋        | 166/960 [00:00<00:00, 1658.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of traning data= 23839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [00:00<00:00, 1387.81it/s]\n",
      "100%|██████████| 111/111 [00:00<00:00, 1153.87it/s]\n",
      "100%|██████████| 1018/1018 [00:00<00:00, 1487.21it/s]\n",
      "100%|██████████| 1825/1825 [00:00<00:00, 1895.34it/s]\n",
      "100%|██████████| 1139/1139 [00:00<00:00, 1726.35it/s]\n",
      "100%|██████████| 797/797 [00:00<00:00, 1633.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5850\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 527,782\n",
      "Trainable params: 526,694\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n",
      "745/745 [==============================] - 930s 1s/step - loss: 1.8662 - accuracy: 0.2906 - val_loss: 1.4325 - val_accuracy: 0.4306\n",
      "745/745 [==============================] - 110s 147ms/step - loss: 1.4299 - accuracy: 0.4251\n",
      "183/183 [==============================] - 27s 146ms/step - loss: 1.4325 - accuracy: 0.4306\n",
      "Train: 0.425, Test: 0.431\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CreateModel():\n",
    "    TRAIN_DATADIR = \"D:\\images\\\\train\"\n",
    "    Validation_DATADIR = \"D:\\images\\\\validation\"\n",
    "    CATEGORIES = [\"angry\",\"disgust\",\"fear\" ,\"happy\",\"sad\",\"surprise\"]\n",
    "    IMG_SIZE = 48\n",
    "    training_data = []\n",
    "    Validation_data=[]\n",
    "    train_datagen = ImageDataGenerator(rotation_range=15,shear_range=0.15,zoom_range=0.15,horizontal_flip=True,)\n",
    "\n",
    "#---------Create the validation Data According to our dataset inputs and labels----------------------\n",
    "    def create_training_data(self):  #Function That Create Training Data Form File\n",
    "        for category in self.CATEGORIES:  # do for all Face Categories\n",
    "\n",
    "            path = os.path.join(self.TRAIN_DATADIR,category)  #create path to Faces\n",
    "            class_num = self.CATEGORIES.index(category)  #get the classification(0 to 5). 0=Angry 1=Disgust....\n",
    "\n",
    "            for img in tqdm(os.listdir(path)):  # iterate over each image per Face Categories\n",
    "                try:\n",
    "                    \n",
    "                    Original_img = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                   \n",
    "                    laplacian =cv2.Laplacian(Original_img,cv2.CV_64F)\n",
    "                    img_array=Original_img-laplacian  \n",
    "                    # new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                    self.training_data.append([img_array, class_num])  # add this to our training_data\n",
    "                except Exception as e:  # in the interest in keeping the output clean...\n",
    "                    pass\n",
    "       #print('lenght of training data : %' % len(self.training_data))    \n",
    " #finally Call Function To Create Train Data\n",
    "\n",
    "\n",
    "#def plotImages(images_arr):\n",
    " #   fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "  #  axes = axes.flatten()\n",
    "   # for img, ax in zip( images_arr, axes):\n",
    "    #    ax.imshow(img)\n",
    "     #   ax.axis('off')\n",
    "        \n",
    "   # plt.tight_layout()\n",
    "   # plt.show()\n",
    "#aug = [training_data[0][0] for i in range(10)]\n",
    "#plotImages(aug) \n",
    "\n",
    "#---------Create the validation Data According to our dataset inputs and labels----------------------\n",
    "    def create_Validation_data(self):\n",
    "        for category in self.CATEGORIES:  # do for all Face Categories\n",
    "            path = os.path.join(self.Validation_DATADIR,category)  #create path to Faces\n",
    "            class_num = self.CATEGORIES.index(category)  #get the classification(0 to 5).0=Angry 1=Disgust ....\n",
    "\n",
    "            for img in tqdm(os.listdir(path)):  # iterate over each image per Face Categories\n",
    "                try:\n",
    "                    Original_img = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                    laplacian =cv2.Laplacian(Original_img,cv2.CV_64F)\n",
    "                    img_array=Original_img-laplacian  \n",
    "                    \n",
    "                    #img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                    #new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                    self.Validation_data.append([img_array, class_num])  # add this to our Validation_data\n",
    "                except Exception as e:  # in the interest in keeping the output clean...\n",
    "                    pass\n",
    "\n",
    "#------------shuffle the Training data And Validation Data----------------\n",
    "#random.shuffle(training_data)#shuffle the training data\n",
    "#random.shuffle(Validation_data)#shuffle the validation data\n",
    "\n",
    "    X = [] #for Input of Train Data\n",
    "    y = [] #for Labels of Train DAta\n",
    "    VAl_X = [] #for Input of Validation_data\n",
    "    VAl_y = [] #for Labels of Validation_data\n",
    "\n",
    "#-----------Split features and labels from training and validation dataset also ------------------------\n",
    "    def split_train_test_data(self):\n",
    "        #----------Store traning and validating dataset features and labels into different variables--------\n",
    "        for features,label in self.training_data:\n",
    "            self.X.append(features)\n",
    "            self.y.append(label)\n",
    "\n",
    "        for features,label in self.Validation_data:\n",
    "            self.VAl_X.append(features)\n",
    "            self.VAl_y.append(label)\n",
    "\n",
    "        # reshape data to have a single channel\n",
    "        #VAl_X = np.array(VAl_X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "        self.y=np.array(self.y)\n",
    "        self.VAl_y=np.array(self.VAl_y)\n",
    "\n",
    "\n",
    "            #-----------------store Encoded Data Set for Futhare use if required Shape the dataset according to your model requirment(input)------------------------\n",
    "\n",
    "        3\n",
    "         #   3pickle_out = open(\"X.pickle\",\"wb\")\n",
    "        #pickle.dump(self.X, pickle_out)\n",
    "        #pickle_out.close()\n",
    "\n",
    "        #pickle_out = open(\"y.pickle\",\"wb\")\n",
    "        #pickle.dump(self.y, pickle_out)\n",
    "        #pickle_out.close()\n",
    "\n",
    "        #pickle_out = open(\"VAl_X.pickle\",\"wb\")\n",
    "        #pickle.dump(self.VAl_X, pickle_out)\n",
    "        #pickle_out.close()\n",
    "\n",
    "        #pickle_out = open(\"VAl_y.pickle\",\"wb\")\n",
    "        #pickle.dump(self.VAl_y, pickle_out)\n",
    "        #pickle_out.close()\n",
    "\n",
    "\n",
    "            #----------------Shape the dataset according to your model requirment(input)--------------\n",
    "        self.X=np.array(self.X)\n",
    "        self.VAl_X = np.array(self.VAl_X)\n",
    "        self.X = self.X.reshape((self.X.shape[0], self.X.shape[1], self.X.shape[2], 1))\n",
    "        self.VAl_X = self.VAl_X.reshape((self.VAl_X.shape[0], self.VAl_X.shape[1], self.VAl_X.shape[2], 1))\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "        self.X=self.X/255.\n",
    "        self.VAl_X = self.VAl_X.astype('float32')\n",
    "        self.VAl_X =self.VAl_X/255.\n",
    "        \n",
    "        self.train_datagen.fit(self.X)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #-----------model definition----------------\n",
    "    #-------------create the model attributes-------------------\n",
    "    def createCnnModel(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu',input_shape=(48, 48, 1)))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))    \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.6))\n",
    "        \n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "       \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.6))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "        #-----------set model compilation attributes------------------------\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        #--------to just see  the Model detials-----------------------------\n",
    "        model.summary()\n",
    "        #-------train the model on data set---------------------------------\n",
    "        #history=model.fit(self.X,self.y, epochs=50,steps_per_epoch=1000,validation_data=(self.VAl_X,self.VAl_y),shuffle=True)#,steps_per_epoch=200\n",
    "        #history=model.fit(self.X,self.y, epochs=5,validation_data=(self.VAl_X,self.VAl_y),shuffle=True)#validation_split=0.1,steps_per_epoch=200\n",
    "\n",
    "        history = model.fit(self.train_datagen.flow(self.X, self.y, batch_size=32),epochs=1,validation_data=(self.VAl_X,self.VAl_y),shuffle=True)\n",
    "    #-------------------Save the model on device------------------------\n",
    "        model.save(\"CNN_Model.h5\")\n",
    "\n",
    "        #-----------------------evaluate the model to show the accuracy and losses-----------\n",
    "        _, train_acc = model.evaluate(self.X,self.y)\n",
    "        _, test_acc = model.evaluate(self.VAl_X, self.VAl_y)\n",
    "        print('Train: %.3f, Test: %.3f' % (train_acc,test_acc))\n",
    "        #----------------------plot loss during training-------------------------------\n",
    "\n",
    "        #---------------------import libery for Plot the Accuracy and losses on graph------------------------\n",
    "        from matplotlib import pyplot\n",
    "\n",
    "        #-------------------------plot losses during training-----------------------------\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Loss')\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        pyplot.plot(history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        #-------------------------plot accuracy during training-----------------------------\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "        \n",
    "createmodel = CreateModel()    \n",
    "createmodel.create_training_data()\n",
    "print('length of traning data= %d' %len(createmodel.training_data))\n",
    "createmodel.create_Validation_data()\n",
    "print(len(createmodel.Validation_data))\n",
    "createmodel.split_train_test_data()\n",
    "createmodel.createCnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-44831d916435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#-----------Read and reshape the Image for prediction-----------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test1.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "    #---------------model prediction------------------------------------------------------\n",
    "    from keras.models import load_model\n",
    "    import cv2 \n",
    "    import numpy as np\n",
    "    from tensorflow.keras.layers import Flatten\n",
    "    from numpy import argmax ,asarray\n",
    "\n",
    "    #--------------Load the saved model from file\n",
    "    model = load_model('Cnnmodel.h5')\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #-----------Read and reshape the Image for prediction-----------------------\n",
    "    img = cv2.imread('test1.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img,(48,48))\n",
    "    img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    classes = model.predict(asarray([img]))\n",
    "    #-----------------print the predicted class for given image-------------------------------\n",
    "    print('Predicted: class=%d' % argmax(classes))\n",
    "    y_classes = classes.argmax(axis=1)\n",
    "    print(CATEGORIES[int(y_classes)])\n",
    "    #----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"SavedDataSet\\X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"SavedDataSet\\y.pickle\",\"rb\")\n",
    "\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"SavedDataSet\\VAl_X.pickle\",\"rb\")\n",
    "\n",
    "VAl_X = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "\n",
    "pickle_in = open(\"SavedDataSet\\VAl_y.pickle\",\"rb\")\n",
    "\n",
    "VAl_y = pickle.load(pickle_in)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1)\n",
      "[[0.8059083  0.00429055 0.1544658  0.01644042 0.01377782 0.00511705]]\n",
      "Predicted: class=0\n",
      "You are : angry with accuracy of : 80 percent\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax ,asarray\n",
    "\n",
    "model = load_model('CNN_Model4.h5')\n",
    "        \n",
    "CATEGORIES = [\"angry\",\"disgust\",\"fear\",\"happy\",\"sad\",\"surprise\"]\n",
    "#CATEGORIES = [\"angry\",\"fear\" ,\"happy\",\"neutral\",\"sad\",\"surprise\"]\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        #-----------Read and reshape the Image for prediction-----------------------\n",
    "        #img = cv2.imread('faces_detected.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "\n",
    "Original_img = cv2.imread('test images/angry.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "laplacian =cv2.Laplacian(Original_img,cv2.CV_64F)\n",
    "img=Original_img-laplacian\n",
    "\n",
    "\n",
    "img = cv2.resize(img,(48,48))\n",
    "img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "img = img.astype('float32') / 255.0\n",
    "\n",
    "shape=img.shape[1:]\n",
    "print(shape)\n",
    "classes = model.predict(np.asarray([img]))\n",
    "        #-----------------print the predicted class for given image-------------------------------\n",
    "print(classes)\n",
    "print('Predicted: class=%d' % np.argmax(classes))   \n",
    "y_classes = classes.argmax(axis=1)\n",
    "print('You are : %s' %CATEGORIES[int(y_classes)] +' with accuracy of : %d percent' %(classes[0][y_classes]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import load_model\n",
    "from PyQt5 import QtWidgets, uic\n",
    "from PyQt5.QtMultimedia import *\n",
    "from matplotlib import pyplot\n",
    "from PyQt5 import QtCore\n",
    "from PyQt5 import QtGui\n",
    "from tqdm import tqdm\n",
    "import import_ipynb\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2 \n",
    "import sys\n",
    "import os\n",
    "from CNN_Model import CNN_Model\n",
    "from Train_Test_Data import train_test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
